---
title: Primal Attention
summary: Self-attention does not need to be resource intensive

date: March 10, 2025
tags:
  - Attention Layer
  - Transformer
---

## Introduction

Self-attention is a powerful mechanism for capturing relationships between tokens in a sequence. However, it can be resource intensive, especially for long sequences.

In this post, we will explore a more efficient approach to self-attention using a combination of sparse attention and local attention.

